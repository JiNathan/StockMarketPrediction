{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the Articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlW2xTdt3YY2",
        "outputId": "95e92738-10d0-4c57-8a1e-bdc60d13daca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import numpy as np\n",
        "#-------USE BELOW CODE ON COLLAB, else comment it out-------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "ehh9BrNx2P8p",
        "outputId": "dab067e5-7fd6-4a65-d572-923cbcc02c69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>published_at</th>\n",
              "      <th>short_description</th>\n",
              "      <th>keywords</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Santoli’s Wednesday market notes: Could Septem...</td>\n",
              "      <td>2021-09-29T17:09:39+0000</td>\n",
              "      <td>This is the daily notebook of Mike Santoli, CN...</td>\n",
              "      <td>cnbc, Premium, Articles, Investment strategy, ...</td>\n",
              "      <td>This is the daily notebook of Mike Santoli, CN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My take on the early Brexit winners and losers</td>\n",
              "      <td>2016-06-24T13:50:48-0400</td>\n",
              "      <td>This commentary originally ran on Facebook. Bo...</td>\n",
              "      <td>Articles, Politics, Europe News, European Cent...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Europe&amp;#039;s recovery depends on Renzi&amp;#039;s...</td>\n",
              "      <td>2014-03-25T13:29:45-0400</td>\n",
              "      <td>In spring, ambitious reforms began in Italy. U...</td>\n",
              "      <td>Articles, Business News, Economy, Europe Econo...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US Moves Closer to Becoming A Major Shareholde...</td>\n",
              "      <td>2009-04-22T19:49:03+0000</td>\n",
              "      <td>The US government is increasingly likely to co...</td>\n",
              "      <td>cnbc, Articles, General Motors Co, Business Ne...</td>\n",
              "      <td>The US government is increasingly likely to co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump: 'Mission accomplished' on 'perfectly ex...</td>\n",
              "      <td>2018-04-14T14:59:04+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cnbc, Articles, George W. Bush, Vladimir Putin...</td>\n",
              "      <td>President Donald Trump hailed the U.S.-led int...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>GLOBAL MARKETS-Euro rises on Spain speculation...</td>\n",
              "      <td>2012-10-02T18:23:00+0000</td>\n",
              "      <td>(Adds comment, details, updates prices)* Spain...</td>\n",
              "      <td>cnbc, Articles, Caterpillar Inc, Europe, Washi...</td>\n",
              "      <td>(Adds comment, details, updates prices)* Spain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>'I come to bury Bitcoin, not to praise it': UBS</td>\n",
              "      <td>2018-11-30T11:38:30+0000</td>\n",
              "      <td>Cryptocurrencies are nearing the end of the ro...</td>\n",
              "      <td>cnbc, Articles, Bitcoin/USD Bitfinex, Economy,...</td>\n",
              "      <td>Cryptocurrencies are nearing the end of the ro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Jon Stewart joins Stephen Colbert to mock that...</td>\n",
              "      <td>2016-07-22T11:44:12+0000</td>\n",
              "      <td>It's been 351 days since Jon Stewart sat behin...</td>\n",
              "      <td>cnbc, Articles, Donald Trump, Media, Elections...</td>\n",
              "      <td>It's been 351 days since Jon Stewart sat behin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Will Stocks Resist 'Anything but Utmost Catast...</td>\n",
              "      <td>2011-11-17T11:50:06+0000</td>\n",
              "      <td>Stock markets have taken such a beating over t...</td>\n",
              "      <td>cnbc, Articles, Business News, Economy, World ...</td>\n",
              "      <td>Stock markets have taken such a beating over t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Biden plans to visit Texas, ask FEMA to accele...</td>\n",
              "      <td>2021-02-19T16:48:01+0000</td>\n",
              "      <td>President Joe Biden said Friday that he plans ...</td>\n",
              "      <td>cnbc, Articles, Breaking News: Politics, Joe B...</td>\n",
              "      <td>President Joe Biden said Friday that he plans ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                title  \\\n",
              "0   Santoli’s Wednesday market notes: Could Septem...   \n",
              "1      My take on the early Brexit winners and losers   \n",
              "2   Europe&#039;s recovery depends on Renzi&#039;s...   \n",
              "3   US Moves Closer to Becoming A Major Shareholde...   \n",
              "4   Trump: 'Mission accomplished' on 'perfectly ex...   \n",
              "..                                                ...   \n",
              "95  GLOBAL MARKETS-Euro rises on Spain speculation...   \n",
              "96   'I come to bury Bitcoin, not to praise it': UBS    \n",
              "97  Jon Stewart joins Stephen Colbert to mock that...   \n",
              "98  Will Stocks Resist 'Anything but Utmost Catast...   \n",
              "99  Biden plans to visit Texas, ask FEMA to accele...   \n",
              "\n",
              "                published_at  \\\n",
              "0   2021-09-29T17:09:39+0000   \n",
              "1   2016-06-24T13:50:48-0400   \n",
              "2   2014-03-25T13:29:45-0400   \n",
              "3   2009-04-22T19:49:03+0000   \n",
              "4   2018-04-14T14:59:04+0000   \n",
              "..                       ...   \n",
              "95  2012-10-02T18:23:00+0000   \n",
              "96  2018-11-30T11:38:30+0000   \n",
              "97  2016-07-22T11:44:12+0000   \n",
              "98  2011-11-17T11:50:06+0000   \n",
              "99  2021-02-19T16:48:01+0000   \n",
              "\n",
              "                                    short_description  \\\n",
              "0   This is the daily notebook of Mike Santoli, CN...   \n",
              "1   This commentary originally ran on Facebook. Bo...   \n",
              "2   In spring, ambitious reforms began in Italy. U...   \n",
              "3   The US government is increasingly likely to co...   \n",
              "4                                                 NaN   \n",
              "..                                                ...   \n",
              "95  (Adds comment, details, updates prices)* Spain...   \n",
              "96  Cryptocurrencies are nearing the end of the ro...   \n",
              "97  It's been 351 days since Jon Stewart sat behin...   \n",
              "98  Stock markets have taken such a beating over t...   \n",
              "99  President Joe Biden said Friday that he plans ...   \n",
              "\n",
              "                                             keywords  \\\n",
              "0   cnbc, Premium, Articles, Investment strategy, ...   \n",
              "1   Articles, Politics, Europe News, European Cent...   \n",
              "2   Articles, Business News, Economy, Europe Econo...   \n",
              "3   cnbc, Articles, General Motors Co, Business Ne...   \n",
              "4   cnbc, Articles, George W. Bush, Vladimir Putin...   \n",
              "..                                                ...   \n",
              "95  cnbc, Articles, Caterpillar Inc, Europe, Washi...   \n",
              "96  cnbc, Articles, Bitcoin/USD Bitfinex, Economy,...   \n",
              "97  cnbc, Articles, Donald Trump, Media, Elections...   \n",
              "98  cnbc, Articles, Business News, Economy, World ...   \n",
              "99  cnbc, Articles, Breaking News: Politics, Joe B...   \n",
              "\n",
              "                                          description  \n",
              "0   This is the daily notebook of Mike Santoli, CN...  \n",
              "1                                                 NaN  \n",
              "2                                                 NaN  \n",
              "3   The US government is increasingly likely to co...  \n",
              "4   President Donald Trump hailed the U.S.-led int...  \n",
              "..                                                ...  \n",
              "95  (Adds comment, details, updates prices)* Spain...  \n",
              "96  Cryptocurrencies are nearing the end of the ro...  \n",
              "97  It's been 351 days since Jon Stewart sat behin...  \n",
              "98  Stock markets have taken such a beating over t...  \n",
              "99  President Joe Biden said Friday that he plans ...  \n",
              "\n",
              "[100 rows x 5 columns]"
            ]
          },
          "execution_count": 257,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fpath = \"cnbc_news_datase.csv\" # \"/content/drive/My Drive/cnbc_news_datase.csv\"\n",
        "\n",
        "#-------Read in article data-------\n",
        "# FILE MUST BE UPLOADED TO YOUR DRIVE; NOTE: if using Collab, also remember to use the second \"/content\" filepath\n",
        "news = pd.read_csv(fpath, usecols = [1, 3, 6,  7, 10])\n",
        "#-------Check data-------\n",
        "news.head(100) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Helper Classes [unused] and Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "KbI9Msb742U_"
      },
      "outputs": [],
      "source": [
        "from helpers import to_time, clear_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load news articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-4FqkBi7mAe",
        "outputId": "60a22c51-b2b9-42b2-99cd-cc50c6f95f32"
      },
      "outputs": [],
      "source": [
        "#-------Globals-------\n",
        "# rename to avoid changing all occurrences just for a different alias\n",
        "articles: pd.DataFrame = news\n",
        "del news\n",
        "\n",
        "#-------Clean articles a bit-------\n",
        "# fix date\n",
        "articles['date'] = articles[\"published_at\"].apply(lambda x: to_time(x))\n",
        "articles = articles.drop(\"published_at\", axis=1)\n",
        "# fix caps\n",
        "str_cols = [\"description\", \"short_description\", \"title\"]\n",
        "articles[str_cols] = articles[str_cols].apply(lambda x: x.str.lower() if x.dtype == 'O' else x)\n",
        "# fix caps in keywords and turn -> np.array\n",
        "articles['keywords'] = articles['keywords'].map(lambda keyword_list: np.array([keyword.lower() for keyword in keyword_list.split(\",\")]))\n",
        "\n",
        "# #-------TEST CODE-------\n",
        "# print(\"Articles about Donald Trump:\")\n",
        "# ex_list = find_relevant_articles(articles, \"Donald Trump\", 0, 200)\n",
        "# print(ex_list.head())\n",
        "# print(f'Example keyword representation {ex_list.iloc[0][\"keywords\"]}')\n",
        "# print(\"\\nArticles about Bitcoin:\")\n",
        "# ex_list = find_relevant_articles(articles, \"cnbc\", 0, 300)\n",
        "# print(ex_list.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter Things outside of the Date Range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "W8bkvTElPD7T",
        "outputId": "a60f4370-b4ec-4690-a70a-4de450c06da2"
      },
      "outputs": [],
      "source": [
        "#-------Constants-------\n",
        "# How many days to keep track of after any given article\n",
        "DAYS_AFTER = 10\n",
        "\n",
        "#-------Download stock market data -> ****hold****-------\n",
        "%run load_stocks.ipynb\n",
        "hold: pd.DataFrame = from_json(\"stocks.json\")\n",
        "# NOTE: Post condition: type(hold) = df.DataFrame(ticker: str, prices: list[float], dates: list[datetime.datetime], industry: str)\n",
        "\n",
        "#--------Process hold/articles on new information-------\n",
        "\n",
        "# find min and max stock data range and filter out articles not in that range\n",
        "all_dates = np.concatenate(hold[\"dates\"])\n",
        "min_date = np.min(all_dates)\n",
        "max_date = np.max(all_dates) - datetime.timedelta(days=DAYS_AFTER)\n",
        "del all_dates\n",
        "\n",
        "articles = articles[(articles['date'] >= min_date) & (articles['date'] <= max_date)].reset_index(drop=True)\n",
        "\n",
        "min_date = np.min(articles['date'])\n",
        "max_date = np.max(articles['date']) - datetime.timedelta(days=DAYS_AFTER)\n",
        "\n",
        "hold['dates'] = hold['dates'].apply(lambda dates: dates if (np.min(dates) <= min_date) & (np.max(dates) >= max_date) else None)\n",
        "hold = hold.dropna()\n",
        "del min_date, max_date\n",
        "# # TEST CODE\n",
        "# hold\n",
        "# len(hold.loc[hold[\"ticker\"] == \"ABNB\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make Price Change Matrix across the Stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [],
      "source": [
        "# speed processing\n",
        "# makes len(stocks) x len(date|price entries) array filled with NaN values\n",
        "max_len = max(len(prices) for prices in hold['prices']) \n",
        "dates_and_prices = np.full((len(hold['prices']), max_len), np.nan)\n",
        "del max_len\n",
        "# populate\n",
        "for i, (dates, prices) in enumerate(zip(hold['dates'], hold['prices'])):\n",
        "    dates_and_prices[i, :len(prices)] = prices\n",
        "# -> df    \n",
        "dates_and_prices = pd.DataFrame(dates_and_prices[:, 1:], columns=hold['dates'][0])\n",
        "\n",
        "# Non-multi-threading attempt\n",
        "def get_date_stocks(date: datetime.datetime) -> pd.DataFrame:\n",
        "    '''Takes a date [from each article] and returns an array of the corresponding stocks until DAYS_AFTER'''\n",
        "    return dates_and_prices.loc[:, \n",
        "            dates_and_prices.columns[\n",
        "                (date <= dates_and_prices.columns) & \n",
        "                (dates_and_prices.columns <= clear_time(date + datetime.timedelta(days=DAYS_AFTER)))\n",
        "                ]\n",
        "            ].diff(axis=1).mean(axis=1)\n",
        "\n",
        "prices = articles[\"date\"].apply(get_date_stocks).reindex()\n",
        "# prices.shape = (511, 442); articles x stocks\n",
        "del dates_and_prices\n",
        "\n",
        "# # Test code\n",
        "# print(prices)\n",
        "# print(get_date_stocks(articles.iloc[0][\"date\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1WPAg-jO_xB"
      },
      "source": [
        "## Do Sentiment Analysis:\n",
        "(From ChatGPT)\n",
        "\n",
        "1. Feature Extraction\n",
        "2. Machine Learning Model Selection:\n",
        "3. Model Training\n",
        "4. Model Evaluation\n",
        "5. Inference:\n",
        "6. Fine-tuning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction\n",
        "### Process\n",
        "\n",
        "1. Vectorize with CountVectorizer(preprocessor=lambda word: re.sub(r'\\b0+', '', word))\n",
        "    -  takes out unneeded 0's at the start of phrases (which the dataset has)\n",
        "2. Run it through a TfidfTransformer to basically assign importance to significant words\n",
        "3. Apply a HTML token filter on a string that gets all the important information from articles\n",
        "4. Fit it the filtered string on the vectorizer and then the TfidfTransformer\n",
        "5. Remove all other columns from articles, and make the columns the vectors for the words over the length of articles\n",
        "\n",
        "Credit to https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "# import re\n",
        "# #-------Vectorize articles, then run a TF-IDF library over it-------\n",
        "# # Make tokenizers\n",
        "# vectorizer = CountVectorizer(preprocessor=lambda word: re.sub(r'\\b0+', '', word))\n",
        "# tfidf_transformer = TfidfTransformer()\n",
        "# # Put everything into col[text] to get all data in one place. get rid of other data as needed\n",
        "# text_row_func = lambda row: re.sub(r'&#[0-9]+;', '', \n",
        "#     f'{row[\"title\"]} {row[\"short_description\"]} {row[\"description\"] if pd.notna(row[\"description\"]) else \"\"} {\" \".join(row[\"keywords\"])}'\n",
        "#     )\n",
        "# # Tokenize\n",
        "# tfidf_matrix = tfidf_transformer.fit_transform(vectorizer.fit_transform(articles.apply(text_row_func, axis=1)))\n",
        "# # Extract column names (i.e., the words)\n",
        "# feature_names = vectorizer.get_feature_names_out()\n",
        "# # Update the articles dataset with the TF-IDF vector on each article\n",
        "# articles = pd.DataFrame(tfidf_matrix.toarray().transpose(), index=vectorizer.get_feature_names_out())\n",
        "# # test a single row (1 x ~20000) vs (~600 x ~20000)\n",
        "# articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>501</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.018</td>\n",
              "      <td>0.013015</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.055794</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01776</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014045</td>\n",
              "      <td>0.044891</td>\n",
              "      <td>0.033208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036904</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017840</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.020623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10004</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zuckerberg</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zuckerman</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zurich</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zyne</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zynga</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053891</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18156 rows × 511 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1    2    3         4         5    6        7    8    \\\n",
              "10          0.018  0.013015  0.0  0.0  0.055794  0.000000  0.0  0.01776  0.0   \n",
              "100         0.000  0.000000  0.0  0.0  0.000000  0.036904  0.0  0.00000  0.0   \n",
              "10004       0.000  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.00000  0.0   \n",
              "1007        0.000  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.00000  0.0   \n",
              "101         0.000  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.00000  0.0   \n",
              "...           ...       ...  ...  ...       ...       ...  ...      ...  ...   \n",
              "zuckerberg  0.000  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.00000  0.0   \n",
              "zuckerman   0.000  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.00000  0.0   \n",
              "zurich      0.000  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.00000  0.0   \n",
              "zyne        0.000  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.00000  0.0   \n",
              "zynga       0.000  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.00000  0.0   \n",
              "\n",
              "            9    ...       501       502       503  504  505  506  507  508  \\\n",
              "10          0.0  ...  0.014045  0.044891  0.033208  0.0  0.0  0.0  0.0  0.0   \n",
              "100         0.0  ...  0.017840  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "10004       0.0  ...  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "1007        0.0  ...  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "101         0.0  ...  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "...         ...  ...       ...       ...       ...  ...  ...  ...  ...  ...   \n",
              "zuckerberg  0.0  ...  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "zuckerman   0.0  ...  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "zurich      0.0  ...  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "zyne        0.0  ...  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "zynga       0.0  ...  0.000000  0.053891  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "            509       510  \n",
              "10          0.0  0.016235  \n",
              "100         0.0  0.020623  \n",
              "10004       0.0  0.000000  \n",
              "1007        0.0  0.000000  \n",
              "101         0.0  0.000000  \n",
              "...         ...       ...  \n",
              "zuckerberg  0.0  0.000000  \n",
              "zuckerman   0.0  0.000000  \n",
              "zurich      0.0  0.000000  \n",
              "zyne        0.0  0.000000  \n",
              "zynga       0.0  0.000000  \n",
              "\n",
              "[18156 rows x 511 columns]"
            ]
          },
          "execution_count": 262,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TfidfVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "#-------Vectorize articles, then run a TF-IDF library over it-------\n",
        "# Make tokenizer\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True, preprocessor=lambda word: re.sub(r'\\b0+', '', word), analyzer='word' , stop_words='english')\n",
        "# Put everything into col[text] to get all data in one place. get rid of other data as needed\n",
        "text_row_func = lambda row: re.sub(r'&#[0-9]+;', '', \n",
        "    f'{row[\"title\"]} {row[\"short_description\"]} {row[\"description\"] if pd.notna(row[\"description\"]) else \"\"} {\" \".join(row[\"keywords\"])}'\n",
        "    )\n",
        "# fit data\n",
        "transform_fit = tfidf_vectorizer.fit_transform(articles.apply(text_row_func, axis=1))\n",
        "articles = pd.DataFrame(transform_fit.toarray().transpose(), tfidf_vectorizer.get_feature_names_out())\n",
        "# shape should be (~20000 x 500)\n",
        "articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Relevancy of each Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transform_fit: (511, 18156) count_docs: (442, 18156)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>ticker</th>\n",
              "      <th>MMM</th>\n",
              "      <th>AOS</th>\n",
              "      <th>ABT</th>\n",
              "      <th>ACN</th>\n",
              "      <th>ADM</th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADP</th>\n",
              "      <th>AES</th>\n",
              "      <th>AFL</th>\n",
              "      <th>A</th>\n",
              "      <th>...</th>\n",
              "      <th>WHR</th>\n",
              "      <th>WMB</th>\n",
              "      <th>WTW</th>\n",
              "      <th>GWW</th>\n",
              "      <th>WYNN</th>\n",
              "      <th>XEL</th>\n",
              "      <th>YUM</th>\n",
              "      <th>ZBRA</th>\n",
              "      <th>ZBH</th>\n",
              "      <th>ZION</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001668</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006676</td>\n",
              "      <td>0.025762</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.005835</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010569</td>\n",
              "      <td>0.003804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003157</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004973</td>\n",
              "      <td>0.003176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017091</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037173</td>\n",
              "      <td>0.262874</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009777</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006273</td>\n",
              "      <td>0.010098</td>\n",
              "      <td>0.032811</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024879</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015838</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049384</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008286</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.060721</td>\n",
              "      <td>0.011253</td>\n",
              "      <td>0.006932</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047223</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>0.034551</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051301</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029447</td>\n",
              "      <td>0.051617</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009310</td>\n",
              "      <td>0.020332</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019139</td>\n",
              "      <td>0.008586</td>\n",
              "      <td>0.097941</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117704</td>\n",
              "      <td>0.006377</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054009</td>\n",
              "      <td>0.072312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021369</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017070</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040916</td>\n",
              "      <td>0.050140</td>\n",
              "      <td>0.010309</td>\n",
              "      <td>0.029784</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001969</td>\n",
              "      <td>0.007681</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>0.020540</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>511 rows × 442 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "ticker       MMM       AOS       ABT       ACN       ADM      ADBE       ADP  \\\n",
              "0       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.001668   \n",
              "1       0.005835  0.000000  0.000000  0.010569  0.003804  0.000000  0.010445   \n",
              "2       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "3       0.000000  0.000000  0.000000  0.000000  0.000000  0.009150  0.000000   \n",
              "4       0.000000  0.009777  0.000000  0.006273  0.010098  0.032811  0.000000   \n",
              "..           ...       ...       ...       ...       ...       ...       ...   \n",
              "506     0.000000  0.000000  0.011471  0.000000  0.000000  0.000000  0.000000   \n",
              "507     0.000000  0.000000  0.000000  0.008286  0.008334  0.060721  0.011253   \n",
              "508     0.034551  0.000000  0.000000  0.000000  0.015016  0.000000  0.000000   \n",
              "509     0.000000  0.000000  0.009310  0.020332  0.000000  0.019139  0.008586   \n",
              "510     0.000000  0.017070  0.000000  0.040916  0.050140  0.010309  0.029784   \n",
              "\n",
              "ticker       AES  AFL         A  ...       WHR       WMB       WTW       GWW  \\\n",
              "0       0.000000  0.0  0.000000  ...  0.000000  0.006676  0.025762  0.000000   \n",
              "1       0.000000  0.0  0.000000  ...  0.003157  0.000000  0.000000  0.004973   \n",
              "2       0.007857  0.0  0.000000  ...  0.000000  0.000000  0.017091  0.000000   \n",
              "3       0.000000  0.0  0.008191  ...  0.000000  0.037173  0.262874  0.000000   \n",
              "4       0.000000  0.0  0.024879  ...  0.000000  0.015838  0.000000  0.000000   \n",
              "..           ...  ...       ...  ...       ...       ...       ...       ...   \n",
              "506     0.000000  0.0  0.000000  ...  0.000000  0.049384  0.000000  0.000000   \n",
              "507     0.006932  0.0  0.047223  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "508     0.000000  0.0  0.000000  ...  0.051301  0.000000  0.000000  0.029447   \n",
              "509     0.097941  0.0  0.000000  ...  0.000000  0.117704  0.006377  0.000000   \n",
              "510     0.000000  0.0  0.007239  ...  0.001969  0.007681  0.000000  0.000000   \n",
              "\n",
              "ticker      WYNN       XEL       YUM      ZBRA  ZBH      ZION  \n",
              "0       0.000000  0.000000  0.000000  0.000000  0.0  0.000000  \n",
              "1       0.003176  0.000000  0.003824  0.000000  0.0  0.007647  \n",
              "2       0.000000  0.000000  0.000000  0.000000  0.0  0.000000  \n",
              "3       0.000000  0.000000  0.000000  0.000000  0.0  0.000000  \n",
              "4       0.000000  0.000000  0.000000  0.037624  0.0  0.000000  \n",
              "..           ...       ...       ...       ...  ...       ...  \n",
              "506     0.000000  0.000000  0.000000  0.000000  0.0  0.000000  \n",
              "507     0.000000  0.000000  0.000000  0.000000  0.0  0.003540  \n",
              "508     0.051617  0.000000  0.062149  0.000000  0.0  0.013672  \n",
              "509     0.054009  0.072312  0.000000  0.021369  0.0  0.000000  \n",
              "510     0.001981  0.000000  0.002385  0.020540  0.0  0.000000  \n",
              "\n",
              "[511 rows x 442 columns]"
            ]
          },
          "execution_count": 267,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# # result should be the same shape as prices\n",
        "# def get_relevancy(keywords: list[str]):\n",
        "#     '''Takes a date [from each article] and returns an array of the corresponding stocks until DAYS_AFTER'''\n",
        "#     print(np.array(articles))\n",
        "#     # print(np.array(articles.columns)[any(keyword in articles.columns for keyword in keywords)])\n",
        "#     # print(articles.loc[:, \n",
        "#     #         articles.columns[any(keyword in articles.columns for keyword in keywords)]\n",
        "#     #         ].mean(axis=1))\n",
        "#     return 1\n",
        "#     # return articles.loc[:, \n",
        "#     #         articles.columns[any(keyword in articles.columns for keyword in keywords)]\n",
        "#     #         ].mean(axis=1)\n",
        "\n",
        "# vectors = vectorizer.fit_transform(hold[\"keywords\"].apply(lambda keywords_list: \" \".join(keywords_list)))\n",
        "# relevancy = tfidf_transformer.transform(vectors)\n",
        "# relevancy = hold[\"keywords\"].apply(lambda doc: transform_fit.transpose(tfidf_vectorizer.transform(doc)))\n",
        "#relevancy = tfidf_vectorizer.transform(hold[\"keywords\"].apply(lambda keywords: \" \".join(keywords)))\n",
        "count_docs = tfidf_vectorizer.transform(hold[\"keywords\"].apply(lambda keywords: \" \".join(keywords)))\n",
        "# relevancy =  hold[\"keywords\"].apply(lambda words: , CountVectorizer) transform_fit.dot()\n",
        "print(f'transform_fit: {transform_fit.shape} count_docs: {count_docs.shape}')\n",
        "relevancy = transform_fit.dot(count_docs.T)\n",
        "relevancy = pd.DataFrame(relevancy.toarray(), columns=hold.index)\n",
        "count_docs\n",
        "relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"None of [Index(['keywords', 'title'], dtype='object')] are in the [columns]\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/news_data_processing.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/news_data_processing.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/news_data_processing.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Define your input (X) and output (y) variables\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/news_data_processing.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X \u001b[39m=\u001b[39m articles[[\u001b[39m'\u001b[39;49m\u001b[39mkeywords\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m]]  \u001b[39m# Assuming 'date' is a datetime column\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/news_data_processing.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Replace 'actual_target_column' with the actual column name you want to predict\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/news_data_processing.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y \u001b[39m=\u001b[39m articles[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: get_date_stocks(row))\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['keywords', 'title'], dtype='object')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "# Data Split. Import y value array from other\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your input (X) and output (y) variables\n",
        "X = articles[['keywords', 'title']]  # Assuming 'date' is a datetime column\n",
        "# Replace 'actual_target_column' with the actual column name you want to predict\n",
        "y = articles['date'].apply(lambda row: get_date_stocks(row))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g-B-3RXiPLYX",
        "outputId": "1895e280-9829-4ebe-9c84-236985133086"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'price_change'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:146\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/index_class_helper.pxi:49\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'price_change'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/News_Data_Processing.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/News_Data_Processing.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Machine Learning Model: Linear Regression\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/News_Data_Processing.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m linear_reg \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/News_Data_Processing.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m linear_reg\u001b[39m.\u001b[39mfit(x_train, y_train[\u001b[39m'\u001b[39;49m\u001b[39mprice_change\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mflatten())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/News_Data_Processing.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saad/Documents/School/CS/SIGNLL_Workshops/NewsSentimentAnalysis/StockMarketPrediction/News_Data_Processing.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m predictions \u001b[39m=\u001b[39m linear_reg\u001b[39m.\u001b[39mpredict(x_test)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'price_change'"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Machine Learning Model: Linear Regression\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(x_train, y_train['price_change'].values.flatten())\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = linear_reg.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f'Mean Squared Error: {mse}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
